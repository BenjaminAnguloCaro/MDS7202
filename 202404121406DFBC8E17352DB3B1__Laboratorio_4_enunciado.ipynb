{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e37cb69cb73a49c2ad07cf670e073cb7",
        "deepnote_cell_height": 156.390625,
        "deepnote_cell_type": "markdown",
        "id": "XUZ1dFPHzAHl"
      },
      "source": [
        "# Laboratorio 4: Spark y EDA üêº\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkEUN6c8S-E_"
      },
      "source": [
        "### Cuerpo Docente:\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebasti√°n Tinoco\n",
        "- Auxiliar: Catherine Benavides\n",
        "- Ayudante: Nicol√°s Ojeda, Eduardo Moya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8ebcb0f2f70c43319279fdd28c13fe89",
        "deepnote_cell_height": 171.796875,
        "deepnote_cell_type": "markdown",
        "id": "tXflExjqzAHr"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados\n",
        "\n",
        "- Nombre de alumno 1: Vanessa Gonz√°lez\n",
        "- Nombre de alumno 2: Benjam√≠n Angulo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "290822720f3e4484b09e762655bcdb76",
        "deepnote_cell_height": 62,
        "deepnote_cell_type": "markdown",
        "id": "AD-V0bbZzAHr"
      },
      "source": [
        "### **Link de repositorio de GitHub:** `https://github.com/BenjaminAnguloCaro/MDS7202/tree/Lab4`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "60255b81ff0349ad9b18f598a8d71386",
        "deepnote_cell_height": 216,
        "deepnote_cell_type": "markdown",
        "id": "hnYD2hBMAwXf",
        "tags": []
      },
      "source": [
        "### Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Fecha de entrega: 7 d√≠as desde la publicaci√≥n, 3 d√≠as de atraso con 1 punto de descuento c/u.\n",
        "- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria.\n",
        "- <u>Prohibidas las copias</u>. Cualquier intento de copia ser√° debidamente penalizado con el reglamento de la escuela.\n",
        "- Tienen que subir el laboratorio a u-cursos Y a su repositorio de github. Labs que no est√©n en u-cursos no ser√°n revisados. Recuerden que el repositorio tambi√©n tiene nota.\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
        "- Pueden usar cualquer material del curso que estimen conveniente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5bf6f5f66dcd4da9a6926774cec108ab",
        "deepnote_cell_height": 114.390625,
        "deepnote_cell_type": "markdown",
        "id": "xzz695obAwXg",
        "tags": []
      },
      "source": [
        "### Temas a tratar\n",
        "\n",
        "- Introducci√≥n al manejo de datos tabulares por medio de la libreria `pandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "50ec30f08f2548a29bc979ed1741f5a0",
        "deepnote_cell_height": 243.390625,
        "deepnote_cell_type": "markdown",
        "id": "6uBLPj1PzAHs"
      },
      "source": [
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Entender, aplicar y aprovechar las ventajas que nos ofrece la libreria `pyspark` para manejar datos tabulares de gran vol√∫men.\n",
        "- Crear gr√°ficos para el desarrollo de An√°lisis de Datos Exploratorios (EDA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7hHEyTgm12s"
      },
      "source": [
        "### Datos del Lab\n",
        "\n",
        "- Base de datos: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/datos_lab_spark.parquet\n",
        "- Objeto serializado: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/object.pkl?inline=false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CrDdk5NRAKe"
      },
      "source": [
        "## Preguntas Te√≥ricas [12 puntos]\n",
        "(2 por pregunta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmDMGUTxLp7M"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://img.buzzfeed.com/buzzfeed-static/static/2018-08/1/17/enhanced/buzzfeed-prod-web-05/anigif_enhanced-9173-1533160033-1.gif\" width=350 />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGZZcxMWRdIa"
      },
      "source": [
        "Responda en  m√°ximo 5 l√≠neas las siguientes preguntas:\n",
        "1. ¬øQu√© es Apache Spark y cu√°les son sus principales ventajas sobre Pandas?\n",
        "2. ¬øQu√© es un RDD en Spark? Describe una de sus principales caracter√≠sticas. ¬øQu√© tienen que ver con los dataframes?.\n",
        "3. Diferencia entre transformaciones y acciones en Spark. Proporciona ejemplos de cada una. ¬øQu√© ocurre internamente cuando se ejecuta una acci√≥n?\n",
        "4. Explica la importancia del particionamiento en Spark y c√≥mo afecta el rendimiento del procesamiento de datos.\n",
        "5. ¬øCu√°les son las funciones de Spark Driver y Spark Executor?\n",
        "6. ¬øQu√© es el Catalyst Optimizer en Apache Spark y cu√°l es su funci√≥n principal en la optimizaci√≥n de consultas SQL?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1elJgE8JRn2O"
      },
      "source": [
        "**Respuestas**\n",
        "\n",
        "> 1.- Apache spark es un framework especializado en el procesamiento de datos a gran escala. De este modo la principal ventaja que tiene este framework por sobre pandas es la capacidad de procesar un gran volumen de datos en tiempo real, adem√°s de que a diferencia de pandas en Apache spark se puede trabajar con data no estructurada, as√≠ como texto o im√°genes. Tambi√©n es importante resaltar el hecho de que Apache spark es m√°s seguro que pandas, ya que cuenta con caracter√≠sticas como la identificaci√≥n, encriptaci√≥n, etc.\n",
        "\n",
        "> 2.- El RDD es la estructura de datos fundamental de Spark y se caracteriza principalmente por ser una estructura inmutable. La relaci√≥n que tienen estos RDD con los dataframes es que ambos son estructuras de datos, pero en el caso de los dataframes se tiene una estructura mutable con tablas similares a las de bases de datos relacionales.\n",
        "\n",
        "> 3.- La diferencia entre las transformaciones y acciones en Spark es que las transformaciones tienen un comportamiento \"lazy\" a diferencia de las acciones, es decir que las funciones que aplican las transformaciones no se ejecutan inmediatamente, sino que quedan en \"cola\" hasta que se ejecute una acci√≥n que desencadene la ejecuci√≥n de las transformaciones. Un ejemplo de transformaci√≥n podr√≠an ser \"map\" o \"filter\" y para el caso de las acciones se tiene algunas como \"reduce\" o \"collect\". Cuando se ejecuta una acci√≥n esta optimiza la ejecuci√≥n de las transformaciones que se est√°n aplicando para garantizar una mayor eficiencia.\n",
        "\n",
        "> 4.-\n",
        "\n",
        "> 5.-\n",
        "\n",
        "> 6.-\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00002-bf13ea5a-d8bf-4cee-879e-ba1c7035e657",
        "deepnote_cell_type": "markdown",
        "id": "b020ce37"
      },
      "source": [
        "## Parte Pr√°ctica\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DaDvtgEYTV"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://pbs.twimg.com/ad_img/1285681293590749189/kDckYy6Z?format=png&name=900x900\" width=350 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW1dg_5_WR8S"
      },
      "source": [
        "Juan Carlos Bodoque, el famoso periodista y empresario, decidi√≥ diversificar su portafolio de negocios y crear su propia plataforma de e-commerce. Despu√©s de varios a√±os de investigar y analizar el mercado financiero, finalmente logr√≥ fundar Bodoque E-Shop con el objetivo de ofrecer a sus clientes una experiencia personalizada y confiable en sus transacciones.\n",
        "\n",
        "Sin embargo, con la llegada de los aliens al planeta Tierra, aparecen nuevos desaf√≠os para el negocio. Por ello, Bodoque decide invertir en un equipo de expertos en tecnolog√≠a y comercio interplanetario, para que Bodoque Shop implemente las √∫ltimas innovaciones en servicio al cliente para garantizar la satisfacci√≥n y fidelizaci√≥n de sus nuevos clientes.\n",
        "\n",
        "El primer objetivo de Bodoque E-Shop ser√° la hacer un an√°lisis exploratorio para entender mejor el comportamiento de los usuarios en la plataforma. Para ello Bodoque les hace entrega de un extenso dataset en el que se registran las actividades que han realizado sus clientes durante los √∫ltimos meses. A continuaci√≥n se presenta un diccionario de variables que levanto el equipo de consultores interplanetarios de Bodoque:\n",
        "\n",
        "1. `Transaction ID`: A unique identifier for each transaction.\n",
        "2. `Customer ID`: A unique identifier for each customer.\n",
        "3. `Transaction Amount`: The total amount of money exchanged in the transaction in USD.\n",
        "4. `Transaction Date`: The date and time when the transaction took place.\n",
        "5. `Payment Method`: The method used to complete the transaction (e.g., credit card, PayPal, etc.).\n",
        "6. `Product Category`: The category of the product involved in the transaction.\n",
        "7. `Quantity`: The number of products involved in the transaction.\n",
        "8. `Customer Age`: The age of the customer making the transaction.\n",
        "9. `Customer Location`: The geographical location of the customer.\n",
        "10. `Device Used`: The type of device used to make the transaction (e.g., mobile, desktop).\n",
        "11. `IP Address`: The IP address of the device used for the transaction.\n",
        "Shipping Address: The address where the product was shipped.\n",
        "12. `Billing Address`: The address associated with the payment method.\n",
        "13. `Is An Alien`: A binary indicator of whether customer is an alien.\n",
        "14. `Account Age Days`: The age of the customer's account in days at the time of the transaction.\n",
        "15. `Transaction Hour`: The hour of the day when the transaction occurred.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1769820f70244385ab5ac51f7509b6de",
        "deepnote_cell_height": 61.133331298828125,
        "deepnote_cell_type": "markdown",
        "id": "MhISwri4zAHy"
      },
      "source": [
        "### Importamos librerias utiles y cargamos los datosüò∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xHoq7VBlJoS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0855897b-e9e1-4497-92ac-83190382aa40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=b05fd7e62de671d40213737f30879e6f40adf06739e795da6dd12e3177df84c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "M6MKzLmPSHzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "394cee7f-0daf-4053-afa5-3c08c433686a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.21.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (24.0)\n",
            "Requirement already satisfied: missingno in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from missingno) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from missingno) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from missingno) (1.11.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from missingno) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->missingno) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn->missingno) (2.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn->missingno) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn->missingno) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Libreria Core del lab.\n",
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
        "from pyspark.sql.functions import when, rand, col, sum\n",
        "\n",
        "#Libreria para plotear\n",
        "!pip install --upgrade plotly\n",
        "!pip install missingno\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vJWSlEXYBqq"
      },
      "source": [
        "Cargue los datos usando **pyspark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "i9Uf-BTZXqXe"
      },
      "outputs": [],
      "source": [
        "# Escriba su respuesta aqu√≠\n",
        "sparksession = SparkSession.builder.master(\"local\").appName(\"Lab 4\").getOrCreate()\n",
        "\n",
        "df = (\n",
        "    sparksession.read\n",
        "    .option(\"header\",'True')\n",
        "    .parquet(\"/content/datos_lab_spark.parquet\")\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6l6GNynYnh4"
      },
      "source": [
        "### 2. Limpieza con pyspark [8 puntos]\n",
        "(1 punto por pregunta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DVdjYyOGRom"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:600/1*A6PpTrehGLxCJWNcUsDTNg.jpeg\" width=350 />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPGV40BjZekP"
      },
      "source": [
        "Para comenzar con el an√°lisis exploratorio usted decide empezar limpiando la base de datos con **pyspark** dado el alto volumen de datos que genera diariamente Bodoque E-Shop.\n",
        "\n",
        "**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCI√ìN**\n",
        "\n",
        "\n",
        "\n",
        "1.   Utilice `.printSchema()` para revisar la estructura de los datos\n",
        "2.   Muestre las primeras 10 filas del dataset. Hint: utilice `.show()`\n",
        "3.   Imprima un muestreo aleatorio con el 5% de los datos diponibles. . Hint: utilice `.sample()`\n",
        "4. Revise los tipos de datos de cada columna con `.dtypes()` y responda la siguiente pregunta: ¬øCu√°l/es columna/s tiene/n un tipo de dato que no es el adecuado y por qu√©?\n",
        "5. Complete el c√≥digo entregado para cambiar el tipo de datos para la/s columna/s problem√°ticas.\n",
        "6. Cuente la cantidad de datos nulos por variable. Recuerde que Spark no posee un m√©todo que le permita calcular directamente los nulos.\n",
        "7. Elimine datos nulos.\n",
        "8. Elimine datos duplicados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nw95Jvr-DtwS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d804174-dc18-4e7f-b866-0ab07dc073a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.- Estructura de los datos: \n",
            "root\n",
            " |-- Transaction ID: string (nullable = true)\n",
            " |-- Customer ID: string (nullable = true)\n",
            " |-- Transaction Amount: double (nullable = true)\n",
            " |-- Transaction Date: timestamp_ntz (nullable = true)\n",
            " |-- Payment Method: string (nullable = true)\n",
            " |-- Product Category: string (nullable = true)\n",
            " |-- Quantity: double (nullable = true)\n",
            " |-- Customer Age: double (nullable = true)\n",
            " |-- Customer Location: string (nullable = true)\n",
            " |-- Device Used: string (nullable = true)\n",
            " |-- IP Address: string (nullable = true)\n",
            " |-- Shipping Address: string (nullable = true)\n",
            " |-- Billing Address: string (nullable = true)\n",
            " |-- Is An Alien: double (nullable = true)\n",
            " |-- Account Age Days: double (nullable = true)\n",
            " |-- Transaction Hour: double (nullable = true)\n",
            "\n",
            "2.- Primeras 10 filas del dataset: \n",
            "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+---------------+--------------------+--------------------+-----------+----------------+----------------+\n",
            "|      Transaction ID|         Customer ID|Transaction Amount|   Transaction Date|Payment Method|Product Category|Quantity|Customer Age|Customer Location|Device Used|     IP Address|    Shipping Address|     Billing Address|Is An Alien|Account Age Days|Transaction Hour|\n",
            "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+---------------+--------------------+--------------------+-----------+----------------+----------------+\n",
            "|4b4a5fe1-ec4d-4f9...|d1b87f62-51b2-493...|             58.09|2022-03-24 00:00:00| bank transfer|     electronics|     1.0|        17.0|    Amandaborough|       NULL| 212.195.49.198|Unit 8934 Box 005...|Unit 8934 Box 005...|        0.0|            NULL|             5.0|\n",
            "|bacd9392-73ce-481...|37de64d5-e901-4a5...|            389.96|2023-01-17 00:00:00|    debit card|     electronics|     2.0|        40.0|     East Timothy|    desktop|208.106.249.121|634 May Keys\\nPor...|634 May Keys\\nPor...|        0.0|            72.0|             8.0|\n",
            "|d936a024-3a1d-40d...|1bac88d6-4b22-409...|            134.19|2023-06-01 00:00:00|        PayPal|   home & garden|     2.0|        22.0|       Davismouth|     tablet|   76.63.88.212|                NULL|16282 Dana Falls ...|        0.0|            63.0|            NULL|\n",
            "|a30aaab5-4042-4b8...|2357c76e-9253-4ce...|            226.17|2019-07-02 00:00:00| bank transfer|        clothing|     5.0|        31.0|         Lynnberg|    desktop| 207.208.171.73|828 Strong Loaf A...|828 Strong Loaf A...|        0.0|           124.0|            20.0|\n",
            "|b0634f43-d07d-4ca...|45071bc5-9588-43e...|            121.53|2020-10-26 00:00:00| bank transfer|        clothing|     2.0|        51.0|             NULL|       NULL| 190.172.14.169|29799 Jason Hills...|29799 Jason Hills...|        0.0|           158.0|            NULL|\n",
            "|8c4711b8-7477-421...|29616b04-2d5c-472...|            166.41|2019-06-10 00:00:00| bank transfer|    toys & games|    NULL|        34.0|     Herreramouth|     tablet|           NULL|5699 Brittany Vil...|120 Kristi Dale\\n...|       NULL|            38.0|            10.0|\n",
            "|fd0342fe-6b16-4a3...|fe21ae29-ba4c-424...|             92.88|2020-10-09 00:00:00|        PayPal|    toys & games|     2.0|        14.0|        Ramosfort|     tablet|   13.45.27.192|                NULL|727 Gibson Island...|        0.0|           119.0|            19.0|\n",
            "|2d7299de-355b-479...|024257c3-5671-4de...|            318.14|2021-10-09 00:00:00|   credit card| health & beauty|     4.0|        42.0|             NULL|    desktop|131.141.230.185|3914 Davis Union\\...|                NULL|        0.0|           251.0|            NULL|\n",
            "|de51c9df-ab52-43a...|                NULL|              NULL|2019-03-22 00:00:00| bank transfer|   home & garden|    NULL|        38.0|       Carneyfurt|       NULL|           NULL|47893 Maldonado S...|                NULL|        0.0|           190.0|            19.0|\n",
            "|                NULL|aab93e75-582f-445...|            121.78|2022-02-28 00:00:00| bank transfer| health & beauty|     4.0|        39.0|       Brockburgh|     mobile| 174.32.252.238|2334 Briana Cente...|2334 Briana Cente...|        0.0|           343.0|            NULL|\n",
            "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+---------------+--------------------+--------------------+-----------+----------------+----------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "3.- Muestreo aleatorio del 5% de los datos: \n",
            "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+---------------+--------------------+--------------------+-----------+----------------+----------------+\n",
            "|      Transaction ID|         Customer ID|Transaction Amount|   Transaction Date|Payment Method|Product Category|Quantity|Customer Age|Customer Location|Device Used|     IP Address|    Shipping Address|     Billing Address|Is An Alien|Account Age Days|Transaction Hour|\n",
            "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+---------------+--------------------+--------------------+-----------+----------------+----------------+\n",
            "|d936a024-3a1d-40d...|1bac88d6-4b22-409...|            134.19|2023-06-01 00:00:00|        PayPal|   home & garden|     2.0|        22.0|       Davismouth|     tablet|   76.63.88.212|                NULL|16282 Dana Falls ...|        0.0|            63.0|            NULL|\n",
            "|                NULL|1967927c-2316-420...|            123.35|2020-03-03 00:00:00|          NULL|    toys & games|    NULL|        35.0| Port Christopher|    desktop|  199.73.51.223|5146 Desiree Ridg...|                NULL|        0.0|            20.0|            NULL|\n",
            "|                NULL|a748a557-cf3d-481...|              NULL|               NULL|   credit card|   home & garden|     2.0|        13.0|         West Amy|     tablet| 123.118.78.180|37175 Christopher...|37175 Christopher...|        0.0|            NULL|             0.0|\n",
            "|836b49d7-fdf3-41a...|82563528-49f7-480...|            153.05|2022-10-18 00:00:00|    debit card|     electronics|     3.0|        27.0|             NULL|     mobile|  94.151.244.55|0554 Stephen Radi...|0554 Stephen Radi...|        0.0|            NULL|             3.0|\n",
            "|b06ef329-60f4-4ca...|a5b3dc58-cff4-441...|            187.37|2019-07-18 00:00:00|        PayPal|            NULL|     4.0|        38.0|       East David|     mobile|           NULL|057 Watson Gatewa...|                NULL|        0.0|            45.0|             1.0|\n",
            "|2f3d0989-aeef-4f0...|                NULL|             31.97|2020-09-23 00:00:00|   credit card|            NULL|     5.0|        38.0|      North Diane|     tablet|   220.17.141.9|1380 Roberts Cany...|1380 Roberts Cany...|        0.0|            NULL|            21.0|\n",
            "|48a83e79-3ef7-4ee...|7a1550d7-1b60-414...|             157.1|2018-06-06 00:00:00|        PayPal|        clothing|     2.0|        28.0| South Sarahville|    desktop|  13.131.100.58|81956 Harold Free...|81956 Harold Free...|        0.0|           207.0|            NULL|\n",
            "|8d6ccbc9-b8c4-448...|8bd99e8e-ea20-446...|            118.67|2023-09-28 00:00:00|   credit card| health & beauty|     3.0|        49.0|      Torresmouth|     tablet|           NULL|85008 Bradshaw Ro...|85008 Bradshaw Ro...|       NULL|           233.0|            15.0|\n",
            "|e686feca-5bfb-410...|                NULL|            569.19|2024-10-19 00:00:00|          NULL|     electronics|    NULL|        NULL|       Karenmouth|     tablet|           NULL|06110 Nicole Expr...|06110 Nicole Expr...|        0.0|           292.0|            20.0|\n",
            "|ab2f01ef-5455-43e...|4e42e7f2-68a8-43a...|            300.22|2024-09-18 00:00:00| bank transfer|            NULL|     2.0|        57.0|       East James|     tablet| 153.198.144.67|4101 Jordan Orcha...|160 Michael Vista...|        0.0|           333.0|            11.0|\n",
            "|                NULL|d657f59f-44ae-485...|             52.05|2019-02-28 00:00:00|    debit card|     electronics|    NULL|        36.0|  South Davidbury|     tablet|           NULL|                NULL|109 Jeffrey Circl...|        0.0|           221.0|            NULL|\n",
            "|48fff70a-e38d-4ef...|                NULL|            296.01|2019-03-02 00:00:00| bank transfer|            NULL|     1.0|        34.0|       Hobbshaven|       NULL|  73.19.167.171|6287 Smith Cliff\\...|6287 Smith Cliff\\...|        0.0|           159.0|             2.0|\n",
            "|7c2d8404-80b6-4a0...|2d46308f-4a94-41c...|             14.01|2020-03-20 00:00:00|        PayPal|     electronics|     5.0|        43.0|       East David|    desktop|175.175.155.249|7354 Vickie Cliff...|7354 Vickie Cliff...|        0.0|           232.0|            18.0|\n",
            "|987d14d7-4fd8-497...|d715916c-5976-4f2...|            108.03|2018-05-24 00:00:00| bank transfer|            NULL|     3.0|        27.0|    East Mikeberg|     mobile|  129.79.69.148|70930 Jefferson C...|70930 Jefferson C...|        0.0|           174.0|            15.0|\n",
            "|251b1b8a-73aa-41b...|70ea408e-1292-45c...|            277.72|2024-10-01 00:00:00|          NULL|            NULL|     2.0|        45.0|    Margaretmouth|    desktop|           NULL|8439 Cervantes Sp...|                NULL|        0.0|            22.0|             3.0|\n",
            "|ab20ec71-4aea-4ca...|dac2f01d-bd13-47b...|             50.86|               NULL|        PayPal|   home & garden|    NULL|        NULL|   New Angelastad|     tablet| 161.213.71.229|18029 Soto Route ...|18029 Soto Route ...|        0.0|           305.0|             9.0|\n",
            "|746f703e-ee1e-4aa...|e9bdcc84-9103-4b0...|            123.72|2024-01-01 00:00:00|        PayPal|        clothing|     4.0|        33.0|        Lynchbury|     mobile|           NULL|438 Marcus Branch...|438 Marcus Branch...|       NULL|           349.0|            11.0|\n",
            "|                NULL|                NULL|            718.84|2024-05-19 00:00:00|    debit card|   home & garden|    NULL|        31.0|       Zamoratown|    desktop| 167.79.191.111|                NULL|USS Brandt\\nFPO A...|        0.0|           170.0|            11.0|\n",
            "|e9582f5f-eff5-463...|d687b41f-5367-4c9...|            165.48|2020-04-24 00:00:00|   credit card|    toys & games|     4.0|        NULL|    New Sarahbury|     mobile|           NULL|                NULL|USCGC King\\nFPO A...|        0.0|            71.0|            16.0|\n",
            "|                NULL|8a0d3531-896c-4d4...|              NULL|2019-07-01 00:00:00| bank transfer|     electronics|     4.0|        NULL|       Davismouth|       NULL|  22.24.244.193|PSC 7759, Box 919...|PSC 7759, Box 919...|        0.0|           325.0|            NULL|\n",
            "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-----------------+-----------+---------------+--------------------+--------------------+-----------+----------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "4.- Revisi√≥n de los tipos de datos de cada columna: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Transaction ID', 'string'),\n",
              " ('Customer ID', 'string'),\n",
              " ('Transaction Amount', 'double'),\n",
              " ('Transaction Date', 'timestamp_ntz'),\n",
              " ('Payment Method', 'string'),\n",
              " ('Product Category', 'string'),\n",
              " ('Quantity', 'double'),\n",
              " ('Customer Age', 'double'),\n",
              " ('Customer Location', 'string'),\n",
              " ('Device Used', 'string'),\n",
              " ('IP Address', 'string'),\n",
              " ('Shipping Address', 'string'),\n",
              " ('Billing Address', 'string'),\n",
              " ('Is An Alien', 'double'),\n",
              " ('Account Age Days', 'double'),\n",
              " ('Transaction Hour', 'double')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Escriba su respuesta aqu√≠\n",
        "print(\"1.- Estructura de los datos: \")\n",
        "df.printSchema() # Se revisa la estructura de los datos\n",
        "print(\"2.- Primeras 10 filas del dataset: \")\n",
        "df.show(10) # Se muestran las primeras 10 filas del dataset\n",
        "print(\"3.- Muestreo aleatorio del 5% de los datos: \")\n",
        "df.sample(fraction=0.05).show() #Se muestra parte del muestreo aleatorio\n",
        "print(\"4.- Revisi√≥n de los tipos de datos de cada columna: \")\n",
        "df.dtypes # Se muestran los tipos de datos de cada columna\n",
        "# Se puede notar que la columna 'is an alien' deber√≠a ser de tipo boolean porque considera √∫nicamente las opciones de s√≠ o no a la pregunta de si es un alien."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"Is An Alien\", col(\"Is An Alien\").cast(\"boolean\"))\n",
        "print(\"5.- Se cambia el tipo de la columna problem√°tica.\")\n",
        "print(\"6.- Cantidad de valores nulos por columna: \")\n",
        "exprs = [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns] #Se calcula la cantidad de valores nulos por columna\n",
        "df.agg(*exprs).show()\n",
        "print(\"7.- Se eliminan los datos con valores nulos: \")\n",
        "print(f\"Cantidad de datos antes de eliminar nulos: {df.count()}\")\n",
        "df = df.na.drop() # Se eliminan los valores nulos\n",
        "print(f\"Cantidad de datos despu√©s de eliminar nulos: {df.count()}\")\n",
        "print(\"8.- Se eliminan los valores duplicados: \")\n",
        "print(f\"Cantidad de datos antes de eliminar duplicados: {df.count()}\")\n",
        "df = df.dropDuplicates([\"Transaction ID\"]) # Se eliminan los valores duplicados de la columna \"Transaction ID\", ya que esta debiese tener valores √∫nicos\n",
        "print(f\"Cantidad de datos despu√©s de eliminar duplicados: {df.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcGbHR0pUQ-0",
        "outputId": "2b2b7f74-42de-40fc-fd1f-dac0e152877b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.- Se cambia el tipo de la columna problem√°tica.\n",
            "6.- Cantidad de valores nulos por columna: \n",
            "+--------------+-----------+------------------+----------------+--------------+----------------+--------+------------+-----------------+-----------+----------+----------------+---------------+-----------+----------------+----------------+\n",
            "|Transaction ID|Customer ID|Transaction Amount|Transaction Date|Payment Method|Product Category|Quantity|Customer Age|Customer Location|Device Used|IP Address|Shipping Address|Billing Address|Is An Alien|Account Age Days|Transaction Hour|\n",
            "+--------------+-----------+------------------+----------------+--------------+----------------+--------+------------+-----------------+-----------+----------+----------------+---------------+-----------+----------------+----------------+\n",
            "|        126886|     126847|            126840|          126831|        126877|          126814|  126966|      126826|           126781|     126911|    126929|          126734|         126804|     126828|          126886|          126953|\n",
            "+--------------+-----------+------------------+----------------+--------------+----------------+--------+------------+-----------------+-----------+----------+----------------+---------------+-----------+----------------+----------------+\n",
            "\n",
            "7.- Se eliminan los datos con valores nulos: \n",
            "Cantidad de datos antes de eliminar nulos: 600000\n",
            "Cantidad de datos despu√©s de eliminar nulos: 13388\n",
            "8.- Se eliminan los valores duplicados: \n",
            "Cantidad de datos antes de eliminar duplicados: 13388\n",
            "Cantidad de datos despu√©s de eliminar duplicados: 13237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjxI2Xd6cRu1"
      },
      "source": [
        "### 3. Transformaciones con pyspark [6 puntos]\n",
        "(1 punto por pregunta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPfhWPZeHXUH"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://live.staticflickr.com/13/91801406_0e71d7f019_b.jpg\" width=350 />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbIDKn44cWhI"
      },
      "source": [
        "**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCI√ìN**\n",
        "\n",
        "Para continuar con el an√°lisis, los especistas de Bodoque les gustar√≠a tener nuevas variables disponibles. Tras las notas de la reuni√≥n usted llega a la conclusi√≥n de que tiene que realizar las siguientes tareas (con el dataset preprocesado de la seccion anterior):\n",
        "\n",
        "\n",
        "1.   Agregar una columna llamada \"Transaction bp\" con el **monto total** de la transacci√≥n en bodoque pesos. Se considera que $x$ d√≥lares equivalen a $log(48+|x^{36}|)$ bodoque pesos.\n",
        "2.   Crear una columna llamada \"Transaction Month\" con el mes en que se realiza una transacci√≥n.\n",
        "3.   Crear la variable *Type of purchase* seg√∫n la catidad de unidades vendidas de acuerdo a las siguientes categor√≠as.\n",
        "  * Compra minorista: 5 productos o menos.\n",
        "  * Compra mayorista: 6 produtos o m√°s.\n",
        "4. Imprima los registros de compras hechas por alien√≠genas en el comecio mayorista.  Utilice `.filter()`.\n",
        "5. Cuente la cantidad de compras realizadas por humanos y la cantidad de compras realizadas por alien√≠genas. Utilice `.groupby()`.\n",
        "6. Muestre una tabla con la recaudaci√≥n promedio por transacci√≥n para cada m√©todo de pago, tanto para humanos como alien√≠genas. Utilice `pivot()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbtFJi3mHnkK"
      },
      "outputs": [],
      "source": [
        "# Escriba su respuesta aqu√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Muj6u2jOLq"
      },
      "source": [
        "### 4. EDA [18 puntos]\n",
        "(1 punto por gr√°fico y 1 punto por su interpretaci√≥n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F3yo66wFQ0z"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://i.pinimg.com/originals/41/7e/7b/417e7b9089bcc20c4909df8954c6e742.gif\" width=400 />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayN5LYRamE7-"
      },
      "source": [
        "Esta secci√≥n tiene como objetivo evaluar su habilidad para generar reportes y conclusiones a partir de los patrones identificados en los datos proporcionados por Bodoque. Espec√≠ficamente, se enfoca en **caracterizar las transacciones** y **explorar las diferencias y similitudes en el comportamiento de humanos y aliens**. Utilice el dataset que ya incluye las transformaciones necesarias.\n",
        "\n",
        "Por favor, aseg√∫rese de que **todas** las visualizaciones que realice cumplan con los siguientes criterios:\n",
        "- Deben ser relevantes y f√°ciles de interpretar.\n",
        "- Cada gr√°fico debe incluir un t√≠tulo claro, nombres en los ejes y leyendas adecuadas.\n",
        "- Adjunte una breve descripci√≥n interpretativa junto a cada gr√°fico para explicar los resultados visualizados.\n",
        "\n",
        "Para llevar a cabo esta tarea, siga los siguientes pasos utilizando la librer√≠a de visualizaci√≥n de su elecci√≥n (matplotlib, seaborn, plotly, etc):\n",
        "\n",
        "1. **Conversi√≥n del DataFrame a formato pandas**: Pase el DataFrame procesado a formato pandas. Evite realizar transformaciones adicionales con pandas.\n",
        "2. **Visualizaci√≥n de Variables Categ√≥ricas**:\n",
        "   - Genere **tres gr√°ficos de barras** que diferencien entre humanos y aliens. Analice y comente cualquier diferencia o similitud observada entre estos dos grupos.\n",
        "3. **Visualizaci√≥n de Variables Num√©ricas**:\n",
        "   - Elabore **tres distplots** para examinar las distribuciones de variables num√©ricas, diferenciando entre humanos y aliens. Comente las diferencias o similitudes notables.\n",
        "4. **An√°lisis de Patrones en Transacciones**:\n",
        "   - Cree **tres gr√°ficos avanzados** que ayuden a identificar patrones en las transacciones. Estos gr√°ficos deben incorporar al menos dos dimensiones y diferir de los anteriores. Algunos ejemplos podr√≠an ser un lineplot que muestre la cantidad de transacciones mensuales por canal de venta, o un barplot que exhiba los tres productos m√°s vendidos por canal.\n",
        "\n",
        "Estos pasos le permitir√°n no solo visualizar datos complejos de manera efectiva, sino tambi√©n interpretar estos datos para extraer insights valiosos acerca del comportamiento de los consumidores en el contexto de Bodoque."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGw5y36IxRk3"
      },
      "outputs": [],
      "source": [
        "# Escriba su respuesta aqu√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97zN2_g4vgY6"
      },
      "source": [
        "### 5. Particiones y consultas en SQL [6 puntos]\n",
        "(2 puntos por pregunta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viNvNuE_odgc"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/1696330143457.gif\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCdHwyGBwVx8"
      },
      "source": [
        "El equipo de Bodoque e-shop ha solicitado que los datos est√©n disponibles en una tabla SQL consultable. Adem√°s, est√°n interesados en aprovechar las funciones de ventana en SQL para an√°lisis avanzados. Las funciones de ventana permiten realizar c√°lculos sobre un conjunto de filas que est√°n relacionadas con la fila actual. Por ejemplo, UNBOUNDED PRECEDING se usa para indicar que el rango de la funci√≥n de ventana comienza desde la primera fila de la partici√≥n o del conjunto de resultados, lo cual es √∫til para calcular sumas acumulativas hasta la fila actual. Las variaciones comunes de este uso incluyen:\n",
        "\n",
        "- `UNBOUNDED PRECEDING` to `CURRENT ROW`: Calcula desde el inicio de la partici√≥n hasta la fila actual.\n",
        "- `UNBOUNDED PRECEDING` to `UNBOUNDED FOLLOWING`: Cubre todas las filas dentro de la partici√≥n.\n",
        "- `VALUE PRECEDING` to `VALUE FOLLOWING`: Establece un rango espec√≠fico basado en valores antes y despu√©s de la fila actual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VntjejKLleIa"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://learnsql.com/blog/sql-window-functions-rows-clause/1.png\" width=500 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8XJ7NrPllKG"
      },
      "source": [
        "Ejemplo de uso en SQL:\n",
        "\n",
        "```sql\n",
        "STAT(COL1_NAME) OVER (PARTITION BY COL2_NAME ORDER BY COL3_NAME ROWS BETWEEN X PRECEDING AND CURRENT ROW)\n",
        "```\n",
        "\n",
        "\n",
        "Responda y realice los siguientes puntos:\n",
        "\n",
        "1. **Creaci√≥n de Tabla con PySpark**:\n",
        "   - Desarrolle un script utilizando PySpark para crear una tabla a partir de un DataFrame previamente transformado. Seleccione y utilice una variable espec√≠fica para la partici√≥n de la tabla. Justifique su elecci√≥n de esta variable considerando factores como el tama√±o del DataFrame, la distribuci√≥n de los datos y el impacto potencial en el rendimiento de futuras consultas.\n",
        "\n",
        "2. **Consulta SQL para Principales Clientes**:\n",
        "   - Ejecute una consulta SQL para identificar los 10 clientes que m√°s productos han comprado. La consulta debe retornar el ID del cliente junto con el total de productos comprados, ordenados en forma descendente.\n",
        "\n",
        "3. **Implementaci√≥n de Funci√≥n de Ventana en SQL y Equivalente en Spark**:\n",
        "   - Implemente una funci√≥n de ventana en SQL para calcular la compra m√°s alta realizada por cada usuario en los √∫ltimos tres meses. Adem√°s, describa c√≥mo se podr√≠a realizar una funci√≥n equivalente en Spark, considerando las capacidades espec√≠ficas de PySpark para manejar este tipo de consultas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe_JQ3npiM6_"
      },
      "outputs": [],
      "source": [
        "# C√≥digo Aqu√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKQs-augfZBv"
      },
      "source": [
        "### 6. UDF [10 puntos]\n",
        "(2 por pregunta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovDBGi-uhhdD"
      },
      "source": [
        "<center>\n",
        "<img src=\"https://64.media.tumblr.com/ba8c705edd2bed0a28d9458811155d69/tumblr_pap19zg4ae1w3zg6go1_400.gifv\" width=400 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJUUnpi8qKHD"
      },
      "source": [
        "\n",
        "\n",
        "Un experto en predicciones y programaci√≥n le ha proporcionado un objeto serializado (`pickle`) dise√±ado para calcular las probabilidades de que un cliente cometa o no un fraude. Este experto sugiere que, para maximizar las capacidades de procesamiento distribuido de Spark, deber√≠a implementar `Scalar User Defined Functions` (udf). Esto le permitir√° aplicar el objeto serializado en un entorno distribuido a lo largo de toda la poblaci√≥n de datos. Un aspecto clave de la funci√≥n desarrollada por el experto es que se enfoca exclusivamente en las siguientes columnas para realizar las predicciones: `['Transaction Amount', 'Quantity', 'Customer Age', 'Transaction Hour']`.\n",
        "\n",
        "Aparte, el experto le proporciona las siguientes instrucciones para usar las UDF en Spark:\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "def custom_function(col):\n",
        "    pass\n",
        "\n",
        "udf_function = udf(custom_function, FloatType())\n",
        "```\n",
        "\n",
        "Bas√°ndose en la estructura proporcionada, debe desarrollar una funci√≥n que ejecute un c√≥digo espec√≠fico. Tenga en cuenta que esta funci√≥n solo puede recibir columnas de Spark y debe retornar el valor deseado. Posteriormente, deber√° utilizar esta funci√≥n UDF indicando la funci√≥n personalizada y el formato de salida.\n",
        "\n",
        "Siga los siguientes pasos para implementar la soluci√≥n y responda las preguntas:\n",
        "\n",
        "1. **Cargar el objeto serializado**: Revise el tipo de objeto y deduzca su funci√≥n.\n",
        "2. **Explorar el objeto**: Utilice las funciones `dir` y `help` para identificar qu√© m√©todo del objeto predice la probabilidad.\n",
        "3. **Crear una funci√≥n personalizada**: Elabore una funci√≥n que prediga la probabilidad de fraude utilizando el √∫ltimo valor de la lista generada por el objeto serializado. Puede modificar el nombre de la funci√≥n para reflejar su prop√≥sito.\n",
        "4. **Definir la funci√≥n UDF**: Establezca la funci√≥n UDF con la funci√≥n personalizada que ha creado.\n",
        "5. **Generar una nueva columna**: A√±ada una nueva columna `prediction` a su DataFrame en Spark utilizando la funci√≥n UDF y muestre un ejemplo de c√≥mo se aplica. ¬øQu√© beneficios podr√≠a generar utilizar udf?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPJVs2OBezN_"
      },
      "outputs": [],
      "source": [
        "# C√≥digo Aqu√≠"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}